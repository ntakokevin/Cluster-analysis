# -*- coding: utf-8 -*-
"""clustering

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cWgtZBGSU7wyKBqSr7Q92iDYLaL-eQx6
"""

from google.colab import drive
drive.mount("/content/drive")

pip install fuzzy-c-means

import pandas as pd 
import seaborn as sns
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from yellowbrick.cluster import KElbowVisualizer,SilhouetteVisualizer
from scipy.cluster import hierarchy
import plotly.graph_objects as go
from fcmeans import FCM



# 
data = pd.read_csv("/content/drive/MyDrive/Bmi-Data.csv")
data.head()

data.describe().T

"""# Data preprocessing

drop duplicate value
"""

data = data.drop_duplicates(keep="first")
data.shape

"""Deal with missing value"""

plt.figure(figsize=(10,6))
sns.displot(
    data=data.isna().melt(value_name="missing"),
    y="variable",
    hue="missing",
    multiple="fill",
    aspect=1.25
)

plt.figure(figsize=(20,10))
data.boxplot()

"""## correlation"""

corr = data.corr()

f, ax = plt.subplots(figsize=(12, 10))
mask = np.triu(np.ones_like(corr, dtype=bool))
cmap = sns.diverging_palette(230, 20, as_cmap=True)
sns.heatmap(corr, annot=True, mask = mask, cmap=cmap)

"""## Standardization"""

data1 = data.drop(['BMI'], axis=1)

data_scaled = StandardScaler().fit_transform(data)
data1_scaled = StandardScaler().fit_transform(data1)

sns.distplot(data_scaled[:,7], hist=True, kde=True, 
             bins=int(180/5), color = 'darkblue', 
             hist_kws={'edgecolor':'black'},
             kde_kws={'linewidth': 4})

"""

```
# Ce texte est au format code
```

### Data reduction  | we will do Principal component Analysis"""

pca = PCA(n_components=5)
df_reduced = pca.fit_transform(data_scaled)

"""## pca.explained_variance_ratio_ gives us the percentage of variance explained by each component."""

print(pca.explained_variance_ratio_)
print(pca.explained_variance_ratio_.sum())

"""### Clusterning"""

wcss = []
n=11
for i in range(1, n):
    kmeans = KMeans(n_clusters=i, random_state=0)
    kmeans.fit(df_reduced)
    wcss.append(kmeans.inertia_)

plt.plot(range(1, n), wcss)
plt.title('Selecting the Numbeer of Clusters using the Elbow Method')
plt.xlabel('Clusters')
plt.ylabel('WCSS')
plt.show()

visualizer = KElbowVisualizer(kmeans, k=(1,10))
 
visualizer.fit(df_reduced)        # Fit the data to the visualizer
visualizer.show()

fig, ax = plt.subplots(3, 2, figsize=(15,8))
for i in [2, 3, 4, 5, 6, 7]:
    km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=42)
    q, mod = divmod(i, 2)
    visualizer = SilhouetteVisualizer(km, colors='yellowbrick', ax=ax[q-1][mod])
    visualizer.fit(df_reduced)

def cluster(data, n_clusters,max_iter=300, fig_title="BMI"):
  kmeans = KMeans(init="random", n_clusters=n_clusters,  n_init=10, max_iter=max_iter, random_state=42)
  kmeans.fit(data)
  label=kmeans.fit_predict(data)
  u_label=np.unique(label)
  centroids=kmeans.cluster_centers_
  plt.title(f"{fig_title} data k-means clustering")
  plt.scatter(data[:,0], data[:,1], c=label, cmap='cividis')
  plt.scatter(centroids[:,0], centroids[:,1], s=200, color='r')

cluster(data=df_reduced, n_clusters=3,max_iter=300)

kmeans.inertia_, kmeans.score(df_reduced)

"""## Visualization

We need to visualize the data to know our clusters.
For that we are focus on the colorrated variable (BMI, BMI_cat, Age, Level,weight).
"""

plt.scatter(data["BMI"], data["BMI_Cat"])

plt.scatter(data["Age"], data["Level"])

plt.scatter(data["BMI"], data["Weight"])

plt.scatter(data["BMI_Cat"], data["Weight"])

"""BMI and Weight are well corralated, mean that our clusters are linked to the BMI and we just need to ckeck the BMI category.
We just need to insert the different data point of each category to know each cluster meanning.
"""

def checkClusterMean(df_reduced_data, n_clusters,max_iter):
  kmeans = KMeans(init="random", n_clusters=n_clusters,  n_init=10, max_iter=max_iter, random_state=42)
  kmeans.fit(df_reduced_data)
  label=kmeans.fit_predict(df_reduced_data)
  u_label=np.unique(label)
  centroids=kmeans.cluster_centers_
  plt.title("BMI data k-means clustering")
  plt.scatter(df_reduced_data[:,0], df_reduced_data[:,1], c=label, cmap='cividis')
  plt.scatter(df_reduced[0,0], df_reduced[0,1], c='g',lw=6,label='data point in over weight cluster')
  plt.scatter(df_reduced[100,0], df_reduced[100,1], c='b',lw=6,label='data point in normal weight cluster')
  plt.scatter(df_reduced[11,0], df_reduced[11,1], c='blue',lw=6,label='data point in under weight cluster')
  plt.scatter(centroids[:,0], centroids[:,1], s=200, color='r',label='centroides')
  plt.legend()

checkClusterMean(df_reduced_data=df_reduced, n_clusters=3,max_iter=300)

"""jaune = surpoids, gris = normal, noir = souspoids

# Hierachical clustering
"""

clusters = hierarchy.linkage(data, method="ward")

plt.figure(figsize=(15, 8))
dendrogram = hierarchy.dendrogram(clusters)
# Plotting a horizontal line based on the first biggest distance between clusters 
plt.axhline(150, color='red', linestyle='--');

from sklearn.preprocessing import normalize
import scipy.cluster.hierarchy as shc
plt.figure(figsize=(10, 7))  
plt.title("Dendrograms")  
dend = shc.dendrogram(shc.linkage(data_scaled, method='ward'))

"""## Fuzzy"""

def fuzzy_cluster_selected(df_reduced,list_n_cluster):
  models=[]
  for cluster in list_n_cluster:
    fcm = FCM(n_clusters=cluster)
    fcm.fit(df_reduced)
    models.append(fcm)
  num_clusters = len(list_n_cluster)
  rows = int(np.ceil(np.sqrt(num_clusters)))
  cols = int(np.ceil(num_clusters/rows))
  f, axe = plt.subplots(rows,cols,figsize=(11,15))
  pces = {}
  for n_cluster , model , axe in zip(list_n_cluster,models,axe.ravel()):
    pc = model.partition_coefficient
    pec = model.partition_entropy_coefficient
    pces[pec] = n_cluster
    fcm_centers = model.centers
    fcm_labels = model.predict(df_reduced)
    # plot 
    axe.scatter(df_reduced[:,0],df_reduced[:,1],c=fcm_labels,alpha=1,cmap='cividis')
    axe.scatter(fcm_centers[:,0],fcm_centers[:,1],marker="o",s=200)
    axe.set_title(f"n_clusters = {n_cluster}, PC ={pc:.3f}, PEC ={pec:.3f}  ")
  key_max = max(sorted(pces.keys()))
  print(f"best cludter number = {pces[key_max]}, pec={key_max} ")
  return pces[key_max], key_max

num_cluster, pec = fuzzy_cluster_selected(df_reduced,list_n_cluster=range(2,11))

print(num_cluster)

def fuzzyCluster(data,num_cluster,plt_title="BMI"):
  fuzzy_model = FCM(n_clusters=num_cluster)
  fuzzy_model.fit(data)
  fcm_centers = fuzzy_model.centers
  fcm_labels = fuzzy_model.predict(data)
  print(np.unique(fcm_labels))
  plt.title(f"{plt_title} fuzzy c-means")
  plt.scatter(data[:,0], data[:,1], c=fcm_labels, cmap='cividis')
  plt.scatter(fcm_centers[:,0], fcm_centers[:,1], s=250, color='r')

fuzzyCluster(df_reduced,num_cluster=3)

"""## Lung dataset"""

data = pd.read_csv("/content/drive/MyDrive/Lung.csv")
data.head(20)

data.shape

"""## Change the categorical value into numeric value"""



"""# Data preprocessing"""

data = data.drop_duplicates(keep="first")
data.shape

"""## Missing values"""

plt.figure(figsize=(10,6))
sns.displot(
    data=data.isna().melt(value_name="missing"),
    y="variable",
    hue="missing",
    multiple="fill",
    aspect=1.25
)



"""## Outliers"""

plt.figure(figsize=(20,10))
data.boxplot()

"""## Correlation"""

corr = data.corr()

f, ax = plt.subplots(figsize=(12, 10))
mask = np.triu(np.ones_like(corr, dtype=bool))
cmap = sns.diverging_palette(230, 20, as_cmap=True)
sns.heatmap(corr, annot=True, mask = mask, cmap=cmap)

"""## Standardization"""

data = data.drop(["GENDER","LUNG_CANCER"], axis=1)

data_scaled = StandardScaler().fit_transform(data)

"""# PCA"""

pca = PCA(n_components=5)
df_reduced = pca.fit_transform(data_scaled)

"""### explained_variance_ratio_"""

print(pca.explained_variance_ratio_)
print(pca.explained_variance_ratio_.sum())

"""### Clustering"""

visualizer = KElbowVisualizer(kmeans, k=(1,10))
visualizer.fit(df_reduced)        # Fit the data to the visualizer
visualizer.show()

fig, ax = plt.subplots(3, 2, figsize=(15,8))
for i in [2, 3, 4, 5, 6, 7]:
    km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=42)
    q, mod = divmod(i, 2)
    visualizer = SilhouetteVisualizer(km, colors='yellowbrick', ax=ax[q-1][mod])
    visualizer.fit(df_reduced)

cluster(data=df_reduced, n_clusters=2, fig_title="Lung")

def checkClusterMean(df_reduced_data, n_clusters,max_iter):
  kmeans = KMeans(init="random", n_clusters=n_clusters,  n_init=10, max_iter=max_iter, random_state=42)
  kmeans.fit(df_reduced_data)
  label=kmeans.fit_predict(df_reduced_data)
  u_label=np.unique(label)
  centroids=kmeans.cluster_centers_
  plt.title("Lung data k-means clustering")
  plt.scatter(df_reduced_data[:,0], df_reduced_data[:,1], c=label, cmap='cividis')
  plt.scatter(df_reduced[18,0], df_reduced[18,1], c='g',lw=6,label='data point in yes lung_cancer')
  plt.scatter(df_reduced[19,0], df_reduced[19,1], c='b',lw=6,label='data point in no lung_cancer')
  plt.scatter(centroids[:,0], centroids[:,1], s=200, color='r',label='centroides')
  plt.legend()

checkClusterMean(df_reduced, n_clusters=2,max_iter=300)

"""## fuzzing cluster"""

num_cluster, pec = fuzzy_cluster_selected(df_reduced,list_n_cluster=range(2,11))

fuzzyCluster(df_reduced,num_cluster=2,plt_title="Lung")

